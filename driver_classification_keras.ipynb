{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Statefarm Distracted Driver Classification (using Keras)\n",
    "## Satchel Grant\n",
    "\n",
    "The goal of this notebook is to classify the statefarm distracted drivers using Keras instead of TensorFlow. I also will implement a generator for data feeding to reduce the memory consumption.\n",
    "\n",
    "### Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "import scipy.misc as sci\n",
    "import time\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def show_img(img):\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read in Data\n",
    "\n",
    "First I read in the file paths of each of the image files and create a parallel array to store the labels and then I shuffle the order of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "external_drive_path = '/Volumes/WhiteElephant/'\n",
    "home_path = os.getcwd()\n",
    "os.chdir(external_drive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples: 22424\n"
     ]
    }
   ],
   "source": [
    "path = './statefarm_drivers/imgs/train'\n",
    "\n",
    "def read_paths(path, no_labels=False):\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    labels_to_nums = dict()\n",
    "    for dir_name, subdir_list, file_list in os.walk(path):\n",
    "        if len(subdir_list) > 0:\n",
    "            label_types = subdir_list\n",
    "            for i,subdir in enumerate(subdir_list):\n",
    "                labels_to_nums[subdir] = i\n",
    "        for img_file in file_list:\n",
    "            if '.jpg' in img_file.lower():\n",
    "                file_paths.append(os.path.join(dir_name,img_file))\n",
    "                if no_labels: labels.append(img_file)\n",
    "                else: labels.append(labels_to_nums[dir_name[-2:]])\n",
    "    if no_labels: return file_paths, labels\n",
    "    n_labels = len(label_types)\n",
    "    return file_paths, labels, n_labels\n",
    "\n",
    "file_paths, labels,n_labels = read_paths(path)\n",
    "file_paths, labels = shuffle(file_paths, labels)\n",
    "print(\"Number of data samples: \" + str(len(file_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, n_classes):\n",
    "    one_hots = []\n",
    "    for label in labels:\n",
    "        one_hot = [0]*n_classes\n",
    "        one_hot[label] = 1\n",
    "        one_hots.append(one_hot)\n",
    "    return np.array(one_hots,dtype=np.float32)\n",
    "\n",
    "labels = one_hot_encode(labels,n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following cell contains some data augmentation functions to increase the amount of useable data. It includes rotations, translations, and combinations of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rotate(img, angle):\n",
    "    rot_img = sci.imrotate(img, angle).astype(img.dtype)\n",
    "    rand_filler = np.random.random(rot_img.size).astype(rot_img.dtype)\n",
    "    rot_img[ones[:,:]!=1] = rand_filler[ones[:,:]!=1]\n",
    "    return rot_img\n",
    "\n",
    "def translate(img, row_amt, col_amt):\n",
    "    translation = np.random.random(img.shape).astype(img.dtype)\n",
    "    if row_amt > 0:\n",
    "        if col_amt > 0:\n",
    "            translation[row_amt:,col_amt:] = img[:-row_amt,:-col_amt]\n",
    "        else:\n",
    "            translation[row_amt:,:-col_amt] = img[:-row_amt,col_amt:]\n",
    "    else:\n",
    "        if col_amt > 0:\n",
    "            translation[:-row_amt,col_amt:] = img[row_amt:,:-col_amt]\n",
    "        else:\n",
    "            translation[:-row_amt,:-col_amt] = img[row_amt:,col_amt:]\n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Next I create a generator to read in the images in batches. This reduces the amount of memory required to deal with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "split_index = int(.75*len(file_paths))\n",
    "X_train_paths, y_train = file_paths[:split_index], labels[:split_index]\n",
    "X_valid_paths, y_valid = file_paths[split_index:], labels[split_index:]\n",
    "batch_size = 128\n",
    "train_steps_per_epoch = len(X_train_paths)//batch_size + 1\n",
    "if len(X_train_paths) % batch_size == 0: train_steps_per_epoch = len(X_train_paths)//batch_size\n",
    "valid_steps = len(X_valid_paths)//batch_size\n",
    "resize_dims = (120,120)\n",
    "\n",
    "\n",
    "def convert_images(paths, resize_dims, img_depth=3):\n",
    "    images = np.empty((len(paths),resize_dims[0],resize_dims[1],img_depth),dtype=np.float32)\n",
    "    for i,path in enumerate(paths):\n",
    "        img = mpimg.imread(path)\n",
    "        images[i] = sci.imresize(img, resize_dims)\n",
    "    return images\n",
    "\n",
    "def image_generator(file_paths, labels, batch_size, resize_dims=(120,120),testing=False,img_depth=3):\n",
    "    while 1:\n",
    "        for batch in range(0, len(file_paths), batch_size):\n",
    "            images = convert_images(file_paths[batch:batch+batch_size],resize_dims,img_depth=img_depth)\n",
    "            batch_labels = labels[batch:batch+batch_size]\n",
    "            if testing: yield images\n",
    "            else: yield images, batch_labels\n",
    "\n",
    "\n",
    "train_generator = image_generator(X_train_paths, y_train, batch_size)\n",
    "valid_generator = image_generator(X_valid_paths, y_valid, len(X_valid_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_valid_batch,y_valid_batch = next(valid_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Input, concatenate, \\\n",
    "        Flatten, Dropout, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model consists of 4 convolutional stacks followed by 2 dense layers. I had good success with this model while predicting the required steering angle from an image of a track for a car to drive around a track in real time. It is also a lightweight model making it quick and easy to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stacks = []\n",
    "conv_shapes = [(1,1),(3,3),(5,5)]\n",
    "conv_depths = [8,10,10,10]\n",
    "pooling_filter = (2,2)\n",
    "pooling_stride = (2,2)\n",
    "dense_shapes = [150,50,n_labels]\n",
    "\n",
    "inputs = Input(shape=(resize_dims[0],resize_dims[1],3))\n",
    "zen_layer = BatchNormalization()(inputs)\n",
    "\n",
    "for shape in conv_shapes:\n",
    "    stacks.append(Conv2D(conv_depths[0], shape, padding='same', activation='elu')(inputs))\n",
    "layer = concatenate(stacks,axis=-1)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling2D(pooling_filter,strides=pooling_stride,padding='same')(layer)\n",
    "layer = Dropout(0.05)(layer)\n",
    "\n",
    "for i in range(1,len(conv_depths)):\n",
    "    stacks = []\n",
    "    for shape in conv_shapes:\n",
    "        stacks.append(Conv2D(conv_depths[i],shape,padding='same',activation='elu')(layer))\n",
    "    layer = concatenate(stacks,axis=-1)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = MaxPooling2D(pooling_filter,strides=pooling_stride, padding='same')(layer)\n",
    "\n",
    "layer = Flatten()(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "\n",
    "for i in range(len(dense_shapes)-1):\n",
    "    layer = Dense(dense_shapes[i], activation='elu')(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "\n",
    "outputs = Dense(dense_shapes[-1], activation='softmax')(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training and Validation\n",
    "The next cell trains the model using the adam optimizer and categorical_crossentropy. The adam optimizer is most efficient because it has specific learning rates for each parameter in the net and it uses momentum. Both of these techniques improves the efficiency of the training process.\n",
    "\n",
    "I use the categorical_crossentropy loss function because this a good loss function for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "model.load_weights('model.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# score = model.evaluate_generator(X_valid_batch, y_valid_batch, batch_size=128, verbose=0)\n",
    "# print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79726 testing images\n"
     ]
    }
   ],
   "source": [
    "path = './statefarm_drivers/imgs/test'\n",
    "test_paths,test_labels = read_paths(path,no_labels=True)\n",
    "print(str(len(test_paths))+' testing images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del file_paths\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_divisions = 4\n",
    "test_generator = image_generator(test_paths,test_labels,len(test_paths)//test_divisions+1,testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "batch_size = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin set 1\n"
     ]
    }
   ],
   "source": [
    "counter+=1\n",
    "base_time = time.time()\n",
    "\n",
    "print('Begin set ' + str(2))\n",
    "X_set = next(test_generator)\n",
    "print('Running time: ' + str(time.time()-base_time)+ 's')\n",
    "\n",
    "n_batches = len(X_set)//batch_size+1\n",
    "if len(X_set)%batch_size == 0: n_batches = len(X_set)//batch_size\n",
    "print('N_batches: '+ str(n_batches))\n",
    "\n",
    "sys.stdout.write(' '*n_batches+'|')\n",
    "for batch in range(0,len(X_set),batch_size):\n",
    "    predictions.append(model.predict_on_batch(X_set[batch:batch+batch_size]))\n",
    "    sys.stdout.write(\"*\")\n",
    "\n",
    "print(\"\\nExecution Time: \" + str((time.time()-base_time)/60)+'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                    ]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b-------------------\n",
      "Execution Time: 13.875528434912363min\n"
     ]
    }
   ],
   "source": [
    "base_time = time.time()\n",
    "# setup toolbar\n",
    "sys.stdout.write(\"[%s]\" % (\" \" * n_batches))\n",
    "sys.stdout.flush()\n",
    "sys.stdout.write(\"\\b\" * (n_batches+1)) # return to start of line, after '['\n",
    "\n",
    "\n",
    "for batch in range(1,n_batches):\n",
    "    predictions.append(model.predict_on_batch(X_set[batch:batch+batch_size]))\n",
    "    sys.stdout.write(\"-\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\nExecution Time: \" + str((time.time()-base_time)/60)+'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin set 4\n",
      "Running time: 356.13717818260193s\n",
      "N_batches: 20\n",
      "[                    ]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b--------------------\n",
      "Execution Time: 19.960290618737538min\n"
     ]
    }
   ],
   "source": [
    "base_time = time.time()\n",
    "print('Begin set ' + str(4))\n",
    "X_set = next(test_generator)\n",
    "print('Running time: ' + str(time.time()-base_time)+ 's')\n",
    "n_batches = len(X_set)//batch_size+1\n",
    "if len(X_set)%batch_size == 0: n_batches = len(X_set)//batch_size\n",
    "print('N_batches: '+ str(n_batches))\n",
    "\n",
    "\n",
    "sys.stdout.write(\"[%s]\" % (\" \" * n_batches))\n",
    "sys.stdout.flush()\n",
    "sys.stdout.write(\"\\n\")\n",
    "\n",
    "for batch in range(n_batches):\n",
    "    predictions.append(model.predict_on_batch(X_set[batch:batch+batch_size]))\n",
    "    sys.stdout.write(\"-\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\nExecution Time: \" + str((time.time()-base_time)/60)+'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for p in predictions:\n",
    "    \n",
    "    print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-cf41515f8c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogit_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mid_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./statefarm_drivers/submission.csv', 'w') as f:\n",
    "    f.write('img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9\\n')\n",
    "    for i,logit_group in enumerate(predictions):\n",
    "        for j,logit in enumerate(logit_group):\n",
    "            id_ = test_labels[i*len(logit_group)+j]\n",
    "            f.write(id_+',')\n",
    "            for k,element in enumerate(logit):\n",
    "                if k == logit.shape[0]-1: f.write(str(element)+'\\n')\n",
    "                else: f.write(str(element)+',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
